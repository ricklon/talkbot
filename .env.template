# =============================================================================
# TalkBot .env.template — copy to .env and fill in machine-specific values
# =============================================================================
# DO NOT commit .env — it contains API keys and machine-specific paths.
# This template documents every variable and the correct value per platform.
# =============================================================================


# -----------------------------------------------------------------------------
# API Keys
# -----------------------------------------------------------------------------

# OpenRouter (required if using openrouter provider)
# Get yours at https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-v1-YOUR_KEY_HERE

# Optional: Hugging Face token for gated/private model access
# Required for private or gated models (e.g. Llama, some TTS models).
# Get yours at https://huggingface.co/settings/tokens
# HF_TOKEN=hf_YOUR_TOKEN_HERE

# Optional: for OpenRouter usage rankings dashboard
OPENROUTER_SITE_URL=https://your-site.com
OPENROUTER_SITE_NAME=TalkBot


# -----------------------------------------------------------------------------
# Default runtime settings
# -----------------------------------------------------------------------------

TALKBOT_MAX_TOKENS=2048
TALKBOT_LLM_PROVIDER=local_server
TALKBOT_DEFAULT_TTS_BACKEND=edge-tts
TALKBOT_DEFAULT_MODEL=qwen3-1.7b-q4_k_m
TALKBOT_DEFAULT_USE_TOOLS=0
TALKBOT_ENABLE_THINKING=0


# -----------------------------------------------------------------------------
# Local llama.cpp provider (TALKBOT_LLM_PROVIDER=local)
# -----------------------------------------------------------------------------

# Path to a GGUF model file (relative to project root or absolute)
TALKBOT_LOCAL_MODEL_PATH=./models/qwen3-1.7b-q4_k_m.gguf

# Context window size for llama.cpp
TALKBOT_LOCAL_N_CTX=8192

# Path to the llama.cpp CLI binary.
#
# PLATFORM NOTES:
#   macOS / Linux:   llama-cli  (must be in PATH, or give absolute path)
#   Windows native:  D:/path/to/tools/llama-cpp/llama-completion.exe
#   Windows + WSL2:  Same as Windows native — point at the .exe or a wrapper.
#                    The benchmark profile sets "llama-cli" as a generic default,
#                    but TALKBOT_LLAMACPP_BIN takes precedence over the profile
#                    value, so set this to avoid FileNotFoundError on Windows.
#
# TALKBOT_LLAMACPP_BIN=llama-cli
# TALKBOT_LLAMACPP_BIN=D:/path/to/tools/llama-cpp/llama-completion.exe


# -----------------------------------------------------------------------------
# Local server provider (TALKBOT_LLM_PROVIDER=local_server)
# Uses any OpenAI-compatible server: Ollama, llama-server, lm-studio, etc.
# -----------------------------------------------------------------------------

# Base URL for the local OpenAI-compatible server.
#
# PLATFORM NOTES:
#   macOS / Linux (Ollama native):      http://localhost:11434/v1
#   Windows native Ollama:              http://localhost:11434/v1
#                                       OR http://127.0.0.1:11434/v1
#   Windows + WSL2 Ollama:              http://<WSL2_IP>:11434/v1
#     ⚠ WSL2 IP changes on every reboot. Find it with:
#       wsl hostname -I   (take the first address, e.g. 172.19.95.193)
#     Then set: TALKBOT_LOCAL_SERVER_URL=http://172.19.95.193:11434/v1
#     ⚠ Do NOT use "localhost" with WSL2 Ollama — on Windows+WSL2, localhost
#       resolves to ::1 (IPv6) first, causing a 5-second timeout before IPv4
#       fallback. Using 127.0.0.1 avoids this for native Windows Ollama, but
#       WSL2 Ollama binds on the WSL2 interface, not the Windows loopback.
#   Remote Ollama over Tailscale:       https://hostname.tailnet-name.ts.net/ollama/v1
#
TALKBOT_LOCAL_SERVER_URL=http://localhost:11434/v1

# Model name/tag sent to the server (defaults to TALKBOT_DEFAULT_MODEL if unset)
# TALKBOT_LOCAL_SERVER_MODEL=llama3.2:3b

# API key for local server if authentication is enabled (usually not needed)
# TALKBOT_LOCAL_SERVER_API_KEY=your_local_server_key


# -----------------------------------------------------------------------------
# Benchmark / runner metadata
# -----------------------------------------------------------------------------

# Short label for this machine in benchmark reports and leaderboard
# Examples: mac-dev, win-dev, linux-ci, mac-m4
TALKBOT_BENCHMARK_RUNNER=dev

# Inference environment label — describes where model inference actually runs.
# This is stored in benchmark results for cross-machine comparison.
#
# Auto-detected when unset, but set explicitly for WSL2 hybrid environments.
#
# Known values:
#   macos-native   — macOS, Ollama or llama.cpp running natively
#   linux-native   — Linux, Ollama or llama.cpp running natively
#   win32-native   — Windows, native Ollama (app or service), no WSL2 involved
#   win32+wsl2     — Windows host, but Ollama runs inside WSL2; models live in WSL2
#   wsl2           — Running from inside WSL2 terminal with Linux Ollama
#
# TALKBOT_INFERENCE_ENV=macos-native
# TALKBOT_INFERENCE_ENV=win32+wsl2
# TALKBOT_INFERENCE_ENV=win32-native


# -----------------------------------------------------------------------------
# HuggingFace / TTS noise suppression (set by tts.py; override here if needed)
# -----------------------------------------------------------------------------

# Suppress HF xet protocol warning (safe to leave on)
HF_HUB_DISABLE_XET=1

# Suppress HF download progress bars (prevents tqdm crash when stdout is
# redirected, e.g. during kittentts initialization in benchmark mode).
# Set to 0 to re-enable progress bars in interactive use.
HF_HUB_DISABLE_PROGRESS_BARS=1


# -----------------------------------------------------------------------------
# Agent system prompt (voice-first assistant behavior)
# Override here to customize without editing source.
# -----------------------------------------------------------------------------
# TALKBOT_AGENT_PROMPT="You are a voice-first assistant..."
