# OpenRouter Configuration
OPENROUTER_API_KEY=your_api_key_here

# Optional: For OpenRouter rankings
OPENROUTER_SITE_URL=https://your-site.com
OPENROUTER_SITE_NAME=TalkBot

# Local-first defaults
TALKBOT_LLM_PROVIDER=local
TALKBOT_DEFAULT_TTS_BACKEND=kittentts
TALKBOT_DEFAULT_MODEL=qwen/qwen3-1.7b
TALKBOT_DEFAULT_USE_TOOLS=1
TALKBOT_LOCAL_MODEL_PATH=./models/default.gguf
TALKBOT_LOCAL_N_CTX=2048
# Optional: OpenAI-compatible local server provider (llama-server / llama_cpp.server)
TALKBOT_LOCAL_SERVER_URL=http://127.0.0.1:8000/v1
# Optional model identifier sent to the server (defaults to local model path or --model)
TALKBOT_LOCAL_SERVER_MODEL=models/default.gguf
# Optional API key for local server if enabled
# TALKBOT_LOCAL_SERVER_API_KEY=your_local_server_key
# Optional if llama-cpp-python is installed.
# Set this only when using external llama.cpp CLI:
# TALKBOT_LLAMACPP_BIN=/full/path/to/llama-cli
TALKBOT_ENABLE_THINKING=0

# Optional: suppress HF xet warning noise when using local HF-backed libs
HF_HUB_DISABLE_XET=1

# Optional: Hugging Face token for gated/private model access
HF_TOKEN=hf_your_token_here
